#import "template.typ": *
#import "@preview/physica:0.9.3": *
#import "@preview/whalogen:0.2.0": ce

#show: project.with(
title: "Delay embedding of Rossler system",
authors: "Rishi Vora"
)

#show link: underline
#set strong(delta: 700)


#grid(
columns: 3,
rows: 3,
gutter: 0.5em,
[Name], [:], [Rishi Vora],
[Reg. No.], [:], [MS21113],
[Code], [:], [https://github.com/Vortriz/IISERM/tree/main/IDC402]
)



= Introduction

Delay coordinate embedding is a technique used in nonlinear dynamics to reconstruct the state space of a system when only a single time series of measurements is available. Often, in real-world systems or experiments, it's not possible to measure all the variables that define the system's state. Instead, we might only have data for one observable quantity over time, which might even be a function of the original state variables rather than one of the variables itself.

It might seem that reducing a complex, high-dimensional system to a single time series would cause a loss of crucial information about the system's dynamics. However, delay embedding allows us to recover many dynamical properties of the original system from this single time series. The core idea stems from the deterministic nature of these systems, where the past influences the future.

= The Method

The procedure involves creating new, multi-dimensional state vectors by combining time-delayed values from the original time series $w(t)$. A vector $v_n​$ in this new, reconstructed space is formed as follows:

$
bold(v)_n ​= (w[n], w[n+tau], ..., w[n + (d−1) tau])
$

Here:

- $w[n]$ is the measurement at time step $n$.
- $tau$ is the delay time (an integer representing time steps).
- $d$ is the embedding dimension (the dimension of the reconstructed space).

This process essentially uses the time series' history to create new dimensions. The collection of all such vectors ${v_n​}$ forms the reconstructed state space, often denoted as $bb(R)$.

= Why it works

The delay embedding relies on the fundamental properties of deterministic dynamical systems and is mathematically grounded by embedding theorems, most famously Takens' Theorem.

In deterministic systems, the current state uniquely determines the future state. This implies that the history of an observable $w(t)$ contains implicit information about the unobserved variables it interacts with. The values of the hidden variables influence the trajectory of the observed variable $w(t)$. By including time-delayed values ($w[n+tau]$, $w[n+2tau]$, etc.) in the reconstructed vector, we are essentially capturing snapshots of this influence over time, allowing us to infer the state of the hidden variables.

Mathematically, the process defines a transformation (a map) $F$ that takes a state $x(t)$ from the original system's attractor $A$ (the set of states the system settles into) and maps it to a vector $v(t)$ in the reconstructed $d$-dimensional space $bb(R)$. This map can be written as:

$
v(t) = & (h(x(t)), h(Phi^(tau_d) ​(x(t))), ..., \ & h(Phi^((d−1)tau d)​(x(t)))) = F(x(t))
$

#figure(
  image("images/schematic.png"),
  caption: [Delay coordinates embedding of a time series generated by the Lorenz-63 system @dysystems]
)

where $h$ is the measurement function and $Phi^tau$ represents the evolution of the original system over time $tau$.

Takens' Theorem (and related results) guarantees that if the original system's attractor $A$ has a fractal dimension $Delta$​, then for almost all choices of smooth measurement functions $h$ and delay times $tau_d$, the map $F$ is an embedding of the original attractor $A$ into the reconstructed space $bb(R)$, provided the embedding dimension $d$ is sufficiently large (specifically, $d > 2 Delta (A)​ $is a sufficient condition for preserving dynamics, while $d > Delta (A)​$ can be enough for just calculating dimensions).

An embedding in this context means the map $F$ is a diffeomorphism between the original attractor $A$ and the reconstructed set $bb(R) = F(A)$. A diffeomorphism is a smooth, invertible map with a smooth inverse and diffeomorphisms preserve topological properties. This means:

- The geometric structure of the attractor is preserved in a topological sense (though it might look stretched or twisted). Points close together on the original attractor remain close in the reconstruction, and vice-versa. Trajectories that don't cross in the original space won't cross in a proper reconstruction.
- Dynamical invariants like fractal dimensions (e.g., correlation dimension, capacity dimension) and Lyapunov exponents are the same for both the original attractor $A$ and the reconstructed set $bb(R)$.

the theorem holds for "almost all" $h$ and $tau_d​$ means that while specific choices might fail (e.g., observing a variable unaffected by others, or choosing a delay equal to a system period, or using a function that collapses symmetries), such cases are exceptional. A generic (typical or randomly chosen) measurement function and delay time will work.

A sufficiently large embedding dimension $d$ is needed to unfold the dynamics properly in the reconstructed space, ensuring that trajectories don't falsely intersect due to projection into too few dimensions. The condition $d > 2 Delta (A)$​ guarantees this unfolding.

#colbreak()

= Optimal time delay and \ embedding dimension

== Choosing the Time Delay ($tau$):

The goal is to choose a $tau$ such that the delayed coordinate $w[n+tau]$ adds the most new information relative to $w[n]$, meaning they should have minimal similarity.

A common approach is to calculate the self-mutual information (SMI) between the time series $w$ and its $tau$-delayed version $w_tau$. The optimal $tau$ is often chosen as the value where the SMI reaches its first minimum. This indicates the point where, on average, $w[n]$ and $w[n+tau]$ share the least information.

Alternatively, one might use the first zero or first minimum of the autocorrelation function of the time series, though this is often less reliable, especially if the autocorrelation decays slowly.

== Choosing the Embedding Dimension (d):

The goal is to find the smallest dimension $d$ that is large enough to fully unfold the dynamics of the attractor in the reconstructed space, eliminating false crossings of trajectories that occur due to projection into too low a dimension.

A widely used method is the False Nearest Neighbors (FNN) algorithm (or variations like AFNN). This method checks how the distance between nearest neighbors in the reconstructed space changes as the dimension is increased from $d$ to $d+1$.

If two points are close in dimension $d$ simply because the attractor projection is squashed, they will often jump far apart when the next dimension $(d+1)$ is added. These are false neighbors.

The optimal embedding dimension $d$ is typically chosen as the dimension where the percentage of false nearest neighbors drops significantly (e.g., close to zero) or where a related statistic ($E_d​$) stops changing substantially and saturates near $1$.

#colbreak()

= Rossler attractor

The equations defining Rossler system @rossler are:

$
dot(x) & = -y - z \
dot(y) & = x + a y \
dot(z) & = b + z (x - c)
$

Properties of this system with $a = 0.1$, $b = 0.1$, and $c = 14$ have been studied extensively.

#figure(
  image("images/rossler.png"),
  caption: [Rossler attractor with $a = 0.1$, $b = 0.1$, and $c = 14$]
)

For embedding, we use:

$
h(x) & = x + "random noise" \
tau & = 100 \
d & = 3
$

These values of $tau$ and $d$ were obtained with DynamicalSystems.jl @dysystems. While we earlier stated that $d > 2 Delta (A)$ is sufficient for embedding, it is not necessary. For Rossler system $d = 3$ suffices. This particular $tau$ was obtained at time step $Delta t = 0.01$.

= Results

We measured Lyapunov exponent of the original system and the delay embedded one, as it is invariant under diffeomorphism.

- Lyapunov (original) = 0.048657
- Lyapunov (delay embedded) = 0.048981

So, we were able to get back our original system with error of just $0.666%$

#colbreak()

= Credits

The simulations were doing using Julia language @julia with the help of following packages:

- OrdinaryDiffEq @diffeq
- CairoMakie @cairo
- DataFrames @df
- DataFramesMeta
- IntervalArithmetic @interval
- CoordinateTransformations
- LinearAlgebra

#bibliography("citations.bib")
